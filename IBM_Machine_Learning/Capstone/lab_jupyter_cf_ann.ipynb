{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML321ENSkillsNetwork32585014-2022-01-01\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Course Rating Prediction using Neural Networks**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **60** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous labs, we have crafted several types of user and item feature vectors.  For example, given a user `i`, we may build its profile feature vector and course rating feature vector, and given an item `j`, we may create its genre vector and user enrollment vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these explicit features vectors, we can perform machine learning tasks such as calculating the similarities among users or items, finding nearest neighbors, and using dot-product to estimate a rating value.\n",
    "\n",
    "The main advantage of using these explicit features is they are highly interpretable and yield very good performance as well. The main disadvantage is we need to spend quite some effort to build and store them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/module\\_4/images/explicit_user_item_features.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it possible to predict a rating without building explicit feature vectors beforehand?\n",
    "\n",
    "Yes, as you may recall, the Non-negative Matrix Factorization decomposes the user-item interaction matrix into user matrix and item matrix, which contain the latent features of users and items and you can simply dot-product them to get an estimated rating.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/module\\_4/images/nmf.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to NMF, neural networks can also be used to extract the latent user and item features?  In fact,  neural networks are very good at learning patterns from data and are widely used to extract latent features.  When training neural networks, it gradually captures and stores the features within its hidden layers as weight matrices and can be extracted to represent the original data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will be training neural networks to predict course ratings while simultaneously extracting users' and items' latent features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing this lab you will be able to:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Use `tensorflow` to train neural networks to extract the user and item latent features from the hidden's layers\n",
    "*   Predict course ratings with trained neural networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and setup lab environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install tensorflow 2.7 if not installed before in your Python environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and import required libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also set a random state\n",
    "rs = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and processing rating dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1889878</td>\n",
       "      <td>CC0101EN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1342067</td>\n",
       "      <td>CL0101EN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1990814</td>\n",
       "      <td>ML0120ENv3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>380098</td>\n",
       "      <td>BD0211EN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>779563</td>\n",
       "      <td>DS0101EN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user        item  rating\n",
       "0  1889878    CC0101EN     3.0\n",
       "1  1342067    CL0101EN     3.0\n",
       "2  1990814  ML0120ENv3     3.0\n",
       "3   380098    BD0211EN     3.0\n",
       "4   779563    DS0101EN     3.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/datasets/ratings.csv\"\n",
    "rating_df = pd.read_csv(rating_url)\n",
    "rating_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same rating dataset we have been using in previous lab, which contains the three main columns: `user`, `item`, and `rating`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's figure out how many unique users and items, their total numbers will determine the sizes of one-hot encoding vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total `33901` of users and `126` items\n"
     ]
    }
   ],
   "source": [
    "num_users = len(rating_df['user'].unique())\n",
    "num_items = len(rating_df['item'].unique())\n",
    "print(f\"There are total `{num_users}` of users and `{num_items}` items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It means our each user can be represented as a `33901 x 1` one-hot vector and each item can be represented as `126 x 1` one-hot vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to create a neural network structure that can take the user and item one-hot vectors as inputs and outputs a rating estimation or the probability of interaction (such as the probability of completing a course).\n",
    "\n",
    "While training and updating the weights in the neural network, its hidden layers should be able to capture the pattern or features for each user and item. Based on this idea, we can design a simple neural network architecture like the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/module\\_4/images/embedding_feature_vector.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network inputs are two one-hot encoding vectors, the blue one is for the user and the green one is for the item. Then on top of them, we added two embedding layers. Here embedding means embedding the one-hot encoding vector into a latent feature space. The embedding layer is a fully-connected layer that outputs the embedding feature vectors. For example, the user embedding layer takes `33901 x 1` one-hot vector as input and outputs a `16 x 1` embedding vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding layer outputs two embedding vectors, which are similar to Non-negative matrix factorization. Then we could simply dot the product the user and item embedding vector to output a rating estimation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the recommender neural network using tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network architecture could be defined and implemented as a sub-class inheriting the `tensorflow.keras.Model` super class, let's call it `RecommenderNet()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommenderNet(keras.Model):\n",
    "    \n",
    "    def __init__(self, num_users, num_items, embedding_size=16, **kwargs):\n",
    "        \"\"\"\n",
    "           Constructor\n",
    "           :param int num_users: number of users\n",
    "           :param int num_items: number of items\n",
    "           :param int embedding_size: the size of embedding vector\n",
    "        \"\"\"\n",
    "        super(RecommenderNet, self).__init__(**kwargs)\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        # Define a user_embedding vector\n",
    "        # Input dimension is the num_users\n",
    "        # Output dimension is the embedding size\n",
    "        self.user_embedding_layer = layers.Embedding(\n",
    "            input_dim=num_users,\n",
    "            output_dim=embedding_size,\n",
    "            name='user_embedding_layer',\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-6),\n",
    "        )\n",
    "        # Define a user bias layer\n",
    "        self.user_bias = layers.Embedding(\n",
    "            input_dim=num_users,\n",
    "            output_dim=1,\n",
    "            name=\"user_bias\")\n",
    "        \n",
    "        # Define an item_embedding vector\n",
    "        # Input dimension is the num_items\n",
    "        # Output dimension is the embedding size\n",
    "        self.item_embedding_layer = layers.Embedding(\n",
    "            input_dim=num_items,\n",
    "            output_dim=embedding_size,\n",
    "            name='item_embedding_layer',\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-6),\n",
    "        )\n",
    "        # Define an item bias layer\n",
    "        self.item_bias = layers.Embedding(\n",
    "            input_dim=num_items,\n",
    "            output_dim=1,\n",
    "            name=\"item_bias\")\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "           method to be called during model fitting\n",
    "           \n",
    "           :param inputs: user and item one-hot vectors\n",
    "        \"\"\"\n",
    "        # Compute the user embedding vector\n",
    "        user_vector = self.user_embedding_layer(inputs[:, 0])\n",
    "        user_bias = self.user_bias(inputs[:, 0])\n",
    "        item_vector = self.item_embedding_layer(inputs[:, 1])\n",
    "        item_bias = self.item_bias(inputs[:, 1])\n",
    "        dot_user_item = tf.tensordot(user_vector, item_vector, 2)\n",
    "        # Add all the components (including bias)\n",
    "        x = dot_user_item + user_bias + item_bias\n",
    "        # Sigmoid output layer to output the probability\n",
    "        return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK: Train and evaluate the RecommenderNet()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to train and evaluate the defined `RecommenderNet()`. First, we need to process the original rating dataset a little bit by converting the actual user ids and item ids into integer indices for `tensorflow` to creating the one-hot encoding vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(raw_data):\n",
    "    \n",
    "    encoded_data = raw_data.copy()\n",
    "    \n",
    "    # Mapping user ids to indices\n",
    "    user_list = encoded_data[\"user\"].unique().tolist()\n",
    "    user_id2idx_dict = {x: i for i, x in enumerate(user_list)}\n",
    "    user_idx2id_dict = {i: x for i, x in enumerate(user_list)}\n",
    "    \n",
    "    # Mapping course ids to indices\n",
    "    course_list = encoded_data[\"item\"].unique().tolist()\n",
    "    course_id2idx_dict = {x: i for i, x in enumerate(course_list)}\n",
    "    course_idx2id_dict = {i: x for i, x in enumerate(course_list)}\n",
    "\n",
    "    # Convert original user ids to idx\n",
    "    encoded_data[\"user\"] = encoded_data[\"user\"].map(user_id2idx_dict)\n",
    "    # Convert original course ids to idx\n",
    "    encoded_data[\"item\"] = encoded_data[\"item\"].map(course_id2idx_dict)\n",
    "    # Convert rating to int\n",
    "    encoded_data[\"rating\"] = encoded_data[\"rating\"].values.astype(\"int\")\n",
    "\n",
    "    return encoded_data, user_idx2id_dict, course_idx2id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data, user_idx2id_dict, course_idx2id_dict = process_dataset(rating_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  item  rating\n",
       "0     0     0       3\n",
       "1     1     1       3\n",
       "2     2     2       3\n",
       "3     3     3       3\n",
       "4     4     4       3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can split the encoded dataset into training and testing datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_test_datasets(dataset, scale=True):\n",
    "\n",
    "    min_rating = min(dataset[\"rating\"])\n",
    "    max_rating = max(dataset[\"rating\"])\n",
    "\n",
    "    dataset = dataset.sample(frac=1, random_state=42)\n",
    "    x = dataset[[\"user\", \"item\"]].values\n",
    "    if scale:\n",
    "        y = dataset[\"rating\"].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values\n",
    "    else:\n",
    "        y = dataset[\"rating\"].values\n",
    "\n",
    "    # Assuming training on 80% of the data and validating on 10%, and testing 10%\n",
    "    train_indices = int(0.8 * dataset.shape[0])\n",
    "    test_indices = int(0.9 * dataset.shape[0])\n",
    "\n",
    "    x_train, x_val, x_test, y_train, y_val, y_test = (\n",
    "        x[:train_indices],\n",
    "        x[train_indices:test_indices],\n",
    "        x[test_indices:],\n",
    "        y[:train_indices],\n",
    "        y[train_indices:test_indices],\n",
    "        y[test_indices:],\n",
    "    )\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, x_test, y_train, y_val, y_test = generate_train_test_datasets(encoded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look at the training input data, it is simply just a list of user indices and item indices, which is a dense format of one-hot encoding vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8376,  7659, 10717, ...,  3409, 28761,  4973], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_indices = x_train[:, 0]\n",
    "user_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 29,  3, ..., 18, 19, 17], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_indices = x_train[:, 1]\n",
    "item_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and training output labels are a list of 0s and 1s indicating if the user has completed a course or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 0., 1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can choose a small embedding vector size to be 16 and create a `RecommenderNet()` model to be trained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 16\n",
    "model = RecommenderNet(num_users, num_items, embedding_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Train the RecommenderNet() model*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "364/365 [============================>.] - ETA: 0s - loss: 0.0086 - root_mean_squared_error: 0.0798\n",
      "Epoch 1: val_loss improved from inf to 0.01755, saving model to model_1\n",
      "INFO:tensorflow:Assets written to: model_1\\assets\n",
      "365/365 [==============================] - 6s 13ms/step - loss: 0.0086 - root_mean_squared_error: 0.0797 - val_loss: 0.0175 - val_root_mean_squared_error: 0.1237\n",
      "Epoch 2/50\n",
      "361/365 [============================>.] - ETA: 0s - loss: 0.0084 - root_mean_squared_error: 0.0782\n",
      "Epoch 2: val_loss improved from 0.01755 to 0.01744, saving model to model_2\n",
      "INFO:tensorflow:Assets written to: model_2\\assets\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0083 - root_mean_squared_error: 0.0781 - val_loss: 0.0174 - val_root_mean_squared_error: 0.1233\n",
      "Epoch 3/50\n",
      "363/365 [============================>.] - ETA: 0s - loss: 0.0079 - root_mean_squared_error: 0.0750\n",
      "Epoch 3: val_loss improved from 0.01744 to 0.01725, saving model to model_3\n",
      "INFO:tensorflow:Assets written to: model_3\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0079 - root_mean_squared_error: 0.0750 - val_loss: 0.0173 - val_root_mean_squared_error: 0.1226\n",
      "Epoch 4/50\n",
      "363/365 [============================>.] - ETA: 0s - loss: 0.0074 - root_mean_squared_error: 0.0718\n",
      "Epoch 4: val_loss improved from 0.01725 to 0.01711, saving model to model_4\n",
      "INFO:tensorflow:Assets written to: model_4\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0074 - root_mean_squared_error: 0.0718 - val_loss: 0.0171 - val_root_mean_squared_error: 0.1220\n",
      "Epoch 5/50\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.0075 - root_mean_squared_error: 0.0725\n",
      "Epoch 5: val_loss improved from 0.01711 to 0.01698, saving model to model_5\n",
      "INFO:tensorflow:Assets written to: model_5\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0075 - root_mean_squared_error: 0.0725 - val_loss: 0.0170 - val_root_mean_squared_error: 0.1215\n",
      "Epoch 6/50\n",
      "359/365 [============================>.] - ETA: 0s - loss: 0.0070 - root_mean_squared_error: 0.0690\n",
      "Epoch 6: val_loss improved from 0.01698 to 0.01685, saving model to model_6\n",
      "INFO:tensorflow:Assets written to: model_6\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0070 - root_mean_squared_error: 0.0694 - val_loss: 0.0168 - val_root_mean_squared_error: 0.1210\n",
      "Epoch 7/50\n",
      "358/365 [============================>.] - ETA: 0s - loss: 0.0068 - root_mean_squared_error: 0.0681\n",
      "Epoch 7: val_loss improved from 0.01685 to 0.01670, saving model to model_7\n",
      "INFO:tensorflow:Assets written to: model_7\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0068 - root_mean_squared_error: 0.0680 - val_loss: 0.0167 - val_root_mean_squared_error: 0.1205\n",
      "Epoch 8/50\n",
      "358/365 [============================>.] - ETA: 0s - loss: 0.0066 - root_mean_squared_error: 0.0667\n",
      "Epoch 8: val_loss improved from 0.01670 to 0.01658, saving model to model_8\n",
      "INFO:tensorflow:Assets written to: model_8\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0066 - root_mean_squared_error: 0.0668 - val_loss: 0.0166 - val_root_mean_squared_error: 0.1201\n",
      "Epoch 9/50\n",
      "360/365 [============================>.] - ETA: 0s - loss: 0.0065 - root_mean_squared_error: 0.0662\n",
      "Epoch 9: val_loss improved from 0.01658 to 0.01643, saving model to model_9\n",
      "INFO:tensorflow:Assets written to: model_9\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0065 - root_mean_squared_error: 0.0662 - val_loss: 0.0164 - val_root_mean_squared_error: 0.1195\n",
      "Epoch 10/50\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.0064 - root_mean_squared_error: 0.0652\n",
      "Epoch 10: val_loss improved from 0.01643 to 0.01615, saving model to model_10\n",
      "INFO:tensorflow:Assets written to: model_10\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0064 - root_mean_squared_error: 0.0652 - val_loss: 0.0162 - val_root_mean_squared_error: 0.1184\n",
      "Epoch 11/50\n",
      "362/365 [============================>.] - ETA: 0s - loss: 0.0062 - root_mean_squared_error: 0.0638\n",
      "Epoch 11: val_loss improved from 0.01615 to 0.01608, saving model to model_11\n",
      "INFO:tensorflow:Assets written to: model_11\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0062 - root_mean_squared_error: 0.0637 - val_loss: 0.0161 - val_root_mean_squared_error: 0.1181\n",
      "Epoch 12/50\n",
      "362/365 [============================>.] - ETA: 0s - loss: 0.0061 - root_mean_squared_error: 0.0630\n",
      "Epoch 12: val_loss did not improve from 0.01608\n",
      "365/365 [==============================] - 3s 8ms/step - loss: 0.0061 - root_mean_squared_error: 0.0630 - val_loss: 0.0161 - val_root_mean_squared_error: 0.1182\n",
      "Epoch 13/50\n",
      "364/365 [============================>.] - ETA: 0s - loss: 0.0059 - root_mean_squared_error: 0.0620\n",
      "Epoch 13: val_loss improved from 0.01608 to 0.01594, saving model to model_13\n",
      "INFO:tensorflow:Assets written to: model_13\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0059 - root_mean_squared_error: 0.0620 - val_loss: 0.0159 - val_root_mean_squared_error: 0.1177\n",
      "Epoch 14/50\n",
      "359/365 [============================>.] - ETA: 0s - loss: 0.0059 - root_mean_squared_error: 0.0617\n",
      "Epoch 14: val_loss improved from 0.01594 to 0.01588, saving model to model_14\n",
      "INFO:tensorflow:Assets written to: model_14\\assets\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0059 - root_mean_squared_error: 0.0617 - val_loss: 0.0159 - val_root_mean_squared_error: 0.1175\n",
      "Epoch 15/50\n",
      "358/365 [============================>.] - ETA: 0s - loss: 0.0057 - root_mean_squared_error: 0.0604\n",
      "Epoch 15: val_loss improved from 0.01588 to 0.01568, saving model to model_15\n",
      "INFO:tensorflow:Assets written to: model_15\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0057 - root_mean_squared_error: 0.0607 - val_loss: 0.0157 - val_root_mean_squared_error: 0.1167\n",
      "Epoch 16/50\n",
      "360/365 [============================>.] - ETA: 0s - loss: 0.0057 - root_mean_squared_error: 0.0603\n",
      "Epoch 16: val_loss improved from 0.01568 to 0.01566, saving model to model_16\n",
      "INFO:tensorflow:Assets written to: model_16\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0057 - root_mean_squared_error: 0.0603 - val_loss: 0.0157 - val_root_mean_squared_error: 0.1168\n",
      "Epoch 17/50\n",
      "359/365 [============================>.] - ETA: 0s - loss: 0.0056 - root_mean_squared_error: 0.0596\n",
      "Epoch 17: val_loss improved from 0.01566 to 0.01559, saving model to model_17\n",
      "INFO:tensorflow:Assets written to: model_17\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0056 - root_mean_squared_error: 0.0597 - val_loss: 0.0156 - val_root_mean_squared_error: 0.1166\n",
      "Epoch 18/50\n",
      "363/365 [============================>.] - ETA: 0s - loss: 0.0055 - root_mean_squared_error: 0.0592\n",
      "Epoch 18: val_loss improved from 0.01559 to 0.01551, saving model to model_18\n",
      "INFO:tensorflow:Assets written to: model_18\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0055 - root_mean_squared_error: 0.0592 - val_loss: 0.0155 - val_root_mean_squared_error: 0.1164\n",
      "Epoch 19/50\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.0054 - root_mean_squared_error: 0.0587\n",
      "Epoch 19: val_loss improved from 0.01551 to 0.01539, saving model to model_19\n",
      "INFO:tensorflow:Assets written to: model_19\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0054 - root_mean_squared_error: 0.0587 - val_loss: 0.0154 - val_root_mean_squared_error: 0.1160\n",
      "Epoch 20/50\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.0053 - root_mean_squared_error: 0.0581\n",
      "Epoch 20: val_loss improved from 0.01539 to 0.01517, saving model to model_20\n",
      "INFO:tensorflow:Assets written to: model_20\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0053 - root_mean_squared_error: 0.0581 - val_loss: 0.0152 - val_root_mean_squared_error: 0.1152\n",
      "Epoch 21/50\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.0052 - root_mean_squared_error: 0.0578\n",
      "Epoch 21: val_loss did not improve from 0.01517\n",
      "365/365 [==============================] - 3s 8ms/step - loss: 0.0052 - root_mean_squared_error: 0.0578 - val_loss: 0.0152 - val_root_mean_squared_error: 0.1154\n",
      "Epoch 22/50\n",
      "361/365 [============================>.] - ETA: 0s - loss: 0.0052 - root_mean_squared_error: 0.0574\n",
      "Epoch 22: val_loss improved from 0.01517 to 0.01508, saving model to model_22\n",
      "INFO:tensorflow:Assets written to: model_22\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0052 - root_mean_squared_error: 0.0574 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1150\n",
      "Epoch 23/50\n",
      "359/365 [============================>.] - ETA: 0s - loss: 0.0051 - root_mean_squared_error: 0.0570\n",
      "Epoch 23: val_loss improved from 0.01508 to 0.01492, saving model to model_23\n",
      "INFO:tensorflow:Assets written to: model_23\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0051 - root_mean_squared_error: 0.0569 - val_loss: 0.0149 - val_root_mean_squared_error: 0.1146\n",
      "Epoch 24/50\n",
      "359/365 [============================>.] - ETA: 0s - loss: 0.0050 - root_mean_squared_error: 0.0567\n",
      "Epoch 24: val_loss did not improve from 0.01492\n",
      "365/365 [==============================] - 3s 8ms/step - loss: 0.0050 - root_mean_squared_error: 0.0568 - val_loss: 0.0149 - val_root_mean_squared_error: 0.1147\n",
      "Epoch 25/50\n",
      "359/365 [============================>.] - ETA: 0s - loss: 0.0049 - root_mean_squared_error: 0.0560\n",
      "Epoch 25: val_loss improved from 0.01492 to 0.01481, saving model to model_25\n",
      "INFO:tensorflow:Assets written to: model_25\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0049 - root_mean_squared_error: 0.0562 - val_loss: 0.0148 - val_root_mean_squared_error: 0.1144\n",
      "Epoch 26/50\n",
      "363/365 [============================>.] - ETA: 0s - loss: 0.0048 - root_mean_squared_error: 0.0560\n",
      "Epoch 26: val_loss improved from 0.01481 to 0.01477, saving model to model_26\n",
      "INFO:tensorflow:Assets written to: model_26\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0048 - root_mean_squared_error: 0.0560 - val_loss: 0.0148 - val_root_mean_squared_error: 0.1144\n",
      "Epoch 27/50\n",
      "364/365 [============================>.] - ETA: 0s - loss: 0.0047 - root_mean_squared_error: 0.0555\n",
      "Epoch 27: val_loss improved from 0.01477 to 0.01473, saving model to model_27\n",
      "INFO:tensorflow:Assets written to: model_27\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0047 - root_mean_squared_error: 0.0556 - val_loss: 0.0147 - val_root_mean_squared_error: 0.1144\n",
      "Epoch 28/50\n",
      "360/365 [============================>.] - ETA: 0s - loss: 0.0047 - root_mean_squared_error: 0.0554\n",
      "Epoch 28: val_loss improved from 0.01473 to 0.01466, saving model to model_28\n",
      "INFO:tensorflow:Assets written to: model_28\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0047 - root_mean_squared_error: 0.0554 - val_loss: 0.0147 - val_root_mean_squared_error: 0.1143\n",
      "Epoch 29/50\n",
      "363/365 [============================>.] - ETA: 0s - loss: 0.0046 - root_mean_squared_error: 0.0550\n",
      "Epoch 29: val_loss improved from 0.01466 to 0.01460, saving model to model_29\n",
      "INFO:tensorflow:Assets written to: model_29\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0046 - root_mean_squared_error: 0.0550 - val_loss: 0.0146 - val_root_mean_squared_error: 0.1143\n",
      "Epoch 30/50\n",
      "363/365 [============================>.] - ETA: 0s - loss: 0.0045 - root_mean_squared_error: 0.0548\n",
      "Epoch 30: val_loss improved from 0.01460 to 0.01454, saving model to model_30\n",
      "INFO:tensorflow:Assets written to: model_30\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0045 - root_mean_squared_error: 0.0548 - val_loss: 0.0145 - val_root_mean_squared_error: 0.1143\n",
      "Epoch 31/50\n",
      "361/365 [============================>.] - ETA: 0s - loss: 0.0044 - root_mean_squared_error: 0.0543\n",
      "Epoch 31: val_loss improved from 0.01454 to 0.01451, saving model to model_31\n",
      "INFO:tensorflow:Assets written to: model_31\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0044 - root_mean_squared_error: 0.0544 - val_loss: 0.0145 - val_root_mean_squared_error: 0.1143\n",
      "Epoch 32/50\n",
      "362/365 [============================>.] - ETA: 0s - loss: 0.0044 - root_mean_squared_error: 0.0542\n",
      "Epoch 32: val_loss improved from 0.01451 to 0.01437, saving model to model_32\n",
      "INFO:tensorflow:Assets written to: model_32\\assets\n",
      "365/365 [==============================] - 4s 10ms/step - loss: 0.0044 - root_mean_squared_error: 0.0543 - val_loss: 0.0144 - val_root_mean_squared_error: 0.1139\n",
      "Epoch 33/50\n",
      "364/365 [============================>.] - ETA: 0s - loss: 0.0043 - root_mean_squared_error: 0.0538\n",
      "Epoch 33: val_loss improved from 0.01437 to 0.01424, saving model to model_33\n",
      "INFO:tensorflow:Assets written to: model_33\\assets\n",
      "365/365 [==============================] - 4s 10ms/step - loss: 0.0043 - root_mean_squared_error: 0.0538 - val_loss: 0.0142 - val_root_mean_squared_error: 0.1135\n",
      "Epoch 34/50\n",
      "364/365 [============================>.] - ETA: 0s - loss: 0.0042 - root_mean_squared_error: 0.0537\n",
      "Epoch 34: val_loss improved from 0.01424 to 0.01410, saving model to model_34\n",
      "INFO:tensorflow:Assets written to: model_34\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0042 - root_mean_squared_error: 0.0536 - val_loss: 0.0141 - val_root_mean_squared_error: 0.1131\n",
      "Epoch 35/50\n",
      "363/365 [============================>.] - ETA: 0s - loss: 0.0041 - root_mean_squared_error: 0.0533\n",
      "Epoch 35: val_loss did not improve from 0.01410\n",
      "365/365 [==============================] - 3s 8ms/step - loss: 0.0041 - root_mean_squared_error: 0.0533 - val_loss: 0.0142 - val_root_mean_squared_error: 0.1136\n",
      "Epoch 36/50\n",
      "360/365 [============================>.] - ETA: 0s - loss: 0.0041 - root_mean_squared_error: 0.0531\n",
      "Epoch 36: val_loss improved from 0.01410 to 0.01407, saving model to model_36\n",
      "INFO:tensorflow:Assets written to: model_36\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0041 - root_mean_squared_error: 0.0532 - val_loss: 0.0141 - val_root_mean_squared_error: 0.1134\n",
      "Epoch 37/50\n",
      "364/365 [============================>.] - ETA: 0s - loss: 0.0040 - root_mean_squared_error: 0.0527\n",
      "Epoch 37: val_loss improved from 0.01407 to 0.01396, saving model to model_37\n",
      "INFO:tensorflow:Assets written to: model_37\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0040 - root_mean_squared_error: 0.0527 - val_loss: 0.0140 - val_root_mean_squared_error: 0.1131\n",
      "Epoch 38/50\n",
      "361/365 [============================>.] - ETA: 0s - loss: 0.0039 - root_mean_squared_error: 0.0526\n",
      "Epoch 38: val_loss improved from 0.01396 to 0.01386, saving model to model_38\n",
      "INFO:tensorflow:Assets written to: model_38\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0039 - root_mean_squared_error: 0.0525 - val_loss: 0.0139 - val_root_mean_squared_error: 0.1129\n",
      "Epoch 39/50\n",
      "363/365 [============================>.] - ETA: 0s - loss: 0.0038 - root_mean_squared_error: 0.0522\n",
      "Epoch 39: val_loss did not improve from 0.01386\n",
      "365/365 [==============================] - 3s 8ms/step - loss: 0.0038 - root_mean_squared_error: 0.0523 - val_loss: 0.0139 - val_root_mean_squared_error: 0.1132\n",
      "Epoch 40/50\n",
      "363/365 [============================>.] - ETA: 0s - loss: 0.0038 - root_mean_squared_error: 0.0522\n",
      "Epoch 40: val_loss improved from 0.01386 to 0.01377, saving model to model_40\n",
      "INFO:tensorflow:Assets written to: model_40\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0038 - root_mean_squared_error: 0.0522 - val_loss: 0.0138 - val_root_mean_squared_error: 0.1129\n",
      "Epoch 41/50\n",
      "359/365 [============================>.] - ETA: 0s - loss: 0.0037 - root_mean_squared_error: 0.0519\n",
      "Epoch 41: val_loss improved from 0.01377 to 0.01376, saving model to model_41\n",
      "INFO:tensorflow:Assets written to: model_41\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0037 - root_mean_squared_error: 0.0519 - val_loss: 0.0138 - val_root_mean_squared_error: 0.1130\n",
      "Epoch 42/50\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.0036 - root_mean_squared_error: 0.0516\n",
      "Epoch 42: val_loss improved from 0.01376 to 0.01359, saving model to model_42\n",
      "INFO:tensorflow:Assets written to: model_42\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0036 - root_mean_squared_error: 0.0516 - val_loss: 0.0136 - val_root_mean_squared_error: 0.1124\n",
      "Epoch 43/50\n",
      "362/365 [============================>.] - ETA: 0s - loss: 0.0036 - root_mean_squared_error: 0.0513\n",
      "Epoch 43: val_loss improved from 0.01359 to 0.01349, saving model to model_43\n",
      "INFO:tensorflow:Assets written to: model_43\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0036 - root_mean_squared_error: 0.0514 - val_loss: 0.0135 - val_root_mean_squared_error: 0.1121\n",
      "Epoch 44/50\n",
      "364/365 [============================>.] - ETA: 0s - loss: 0.0035 - root_mean_squared_error: 0.0512\n",
      "Epoch 44: val_loss improved from 0.01349 to 0.01346, saving model to model_44\n",
      "INFO:tensorflow:Assets written to: model_44\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0035 - root_mean_squared_error: 0.0511 - val_loss: 0.0135 - val_root_mean_squared_error: 0.1122\n",
      "Epoch 45/50\n",
      "362/365 [============================>.] - ETA: 0s - loss: 0.0035 - root_mean_squared_error: 0.0509\n",
      "Epoch 45: val_loss improved from 0.01346 to 0.01344, saving model to model_45\n",
      "INFO:tensorflow:Assets written to: model_45\\assets\n",
      "365/365 [==============================] - 4s 10ms/step - loss: 0.0035 - root_mean_squared_error: 0.0509 - val_loss: 0.0134 - val_root_mean_squared_error: 0.1122\n",
      "Epoch 46/50\n",
      "362/365 [============================>.] - ETA: 0s - loss: 0.0034 - root_mean_squared_error: 0.0507\n",
      "Epoch 46: val_loss improved from 0.01344 to 0.01343, saving model to model_46\n",
      "INFO:tensorflow:Assets written to: model_46\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0034 - root_mean_squared_error: 0.0506 - val_loss: 0.0134 - val_root_mean_squared_error: 0.1123\n",
      "Epoch 47/50\n",
      "360/365 [============================>.] - ETA: 0s - loss: 0.0033 - root_mean_squared_error: 0.0503\n",
      "Epoch 47: val_loss improved from 0.01343 to 0.01327, saving model to model_47\n",
      "INFO:tensorflow:Assets written to: model_47\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0033 - root_mean_squared_error: 0.0505 - val_loss: 0.0133 - val_root_mean_squared_error: 0.1118\n",
      "Epoch 48/50\n",
      "361/365 [============================>.] - ETA: 0s - loss: 0.0033 - root_mean_squared_error: 0.0502\n",
      "Epoch 48: val_loss did not improve from 0.01327\n",
      "365/365 [==============================] - 3s 8ms/step - loss: 0.0033 - root_mean_squared_error: 0.0503 - val_loss: 0.0133 - val_root_mean_squared_error: 0.1120\n",
      "Epoch 49/50\n",
      "364/365 [============================>.] - ETA: 0s - loss: 0.0032 - root_mean_squared_error: 0.0500\n",
      "Epoch 49: val_loss improved from 0.01327 to 0.01319, saving model to model_49\n",
      "INFO:tensorflow:Assets written to: model_49\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0032 - root_mean_squared_error: 0.0500 - val_loss: 0.0132 - val_root_mean_squared_error: 0.1117\n",
      "Epoch 50/50\n",
      "364/365 [============================>.] - ETA: 0s - loss: 0.0032 - root_mean_squared_error: 0.0497\n",
      "Epoch 50: val_loss improved from 0.01319 to 0.01317, saving model to model_50\n",
      "INFO:tensorflow:Assets written to: model_50\\assets\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0032 - root_mean_squared_error: 0.0497 - val_loss: 0.0132 - val_root_mean_squared_error: 0.1118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2149a7459f0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"model_{epoch}\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=1,\n",
    "    )\n",
    "]\n",
    "\n",
    "## - call model.compile() method to set up the loss and optimizer and metrics for the model training, you may use\n",
    "##  - - tf.keras.losses.MeanSquaredError() as training loss\n",
    "##  - - keras.optimizers.Adam() as optimizer\n",
    "##  - - tf.keras.metrics.RootMeanSquaredError() as metric\n",
    "optimizer = optimizers.Adam(learning_rate=1e-4)\n",
    "mse = MeanSquaredError()\n",
    "batchsize=512\n",
    "\n",
    "model.compile(loss=mse,\n",
    "              optimizer=optimizer,\n",
    "              metrics=[RootMeanSquaredError()]\n",
    "             )\n",
    "\n",
    "## - call model.fit() to train the model\n",
    "model_fit = model.fit(x_train,\n",
    "                      y_train,\n",
    "                      batch_size=batchsize,\n",
    "                      epochs=50,\n",
    "                      callbacks=callbacks,\n",
    "                      validation_data=(x_val, y_val)\n",
    "                     )\n",
    "\n",
    "## - optionally call model.save() to save the model\n",
    "\n",
    "## - plot the train and validation loss\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(model_fit.history['loss'], label='train loss')\n",
    "plt.plot(model_fit.history['val_loss'], label='validation loss')\n",
    "plt.title('Epoch vs Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO:* Evaluate the trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 5ms/step - loss: 0.0130 - root_mean_squared_error: 0.1108\n",
      "test loss, test rmse:  [0.01297159492969513, 0.1108463853597641]\n"
     ]
    }
   ],
   "source": [
    "### - call model.evaluate() to evaluate the model\n",
    "results = model.evaluate(x_test, y_test, batch_size = batchsize)\n",
    "print(\"test loss, test rmse: \", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the user and item embedding vectors as latent feature vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have trained the `RecommenderNet()` model and it can predict the ratings with relatively small RMSE.\n",
    "\n",
    "If we print the trained model then we can see its layers and their parameters/weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"recommender_net\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " user_embedding_layer (Embed  multiple                 542416    \n",
      " ding)                                                           \n",
      "                                                                 \n",
      " user_bias (Embedding)       multiple                  33901     \n",
      "                                                                 \n",
      " item_embedding_layer (Embed  multiple                 2016      \n",
      " ding)                                                           \n",
      "                                                                 \n",
      " item_bias (Embedding)       multiple                  126       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 578,459\n",
      "Trainable params: 578,459\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `RecommenderNet`, the `user_embedding_layer` and `item_embedding_layer` layers contain the trained weights. Essentially, they are the latent user and item features learned by `RecommenderNet` and will be used to predict the interaction. As such, while training the neural network to predict rating, the embedding layers are simultaneously trained to extract the embedding user and item features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily get the actual weights using `model.get_layer().get_weights()` methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User features shape: (33901, 16)\n"
     ]
    }
   ],
   "source": [
    "# User features\n",
    "user_latent_features = model.get_layer('user_embedding_layer').get_weights()[0]\n",
    "print(f\"User features shape: {user_latent_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03790464, -0.03878061,  0.07414042, -0.0095172 ,  0.0215273 ,\n",
       "       -0.02508512, -0.03361789, -0.05500112, -0.01313655, -0.07372444,\n",
       "       -0.01991272,  0.02667735, -0.08372416,  0.01670171,  0.02174026,\n",
       "        0.01732759], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_latent_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item features shape: (126, 16)\n"
     ]
    }
   ],
   "source": [
    "item_latent_features = model.get_layer('item_embedding_layer').get_weights()[0]\n",
    "print(f\"Item features shape: {item_latent_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.0709016e-03,  7.3820946e-04, -5.4260669e-05, -1.2020412e-03,\n",
       "       -2.0423196e-03,  1.8122952e-04,  2.6858042e-03, -1.3213635e-03,\n",
       "       -6.9396215e-04, -2.4925154e-03, -1.7116501e-03, -1.5066146e-03,\n",
       "       -1.6458227e-03, -7.1490527e-04,  3.5440277e-03, -2.1892600e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_latent_features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, each user of the total 33901 users has been transformed into a 16 x 1 latent feature vector and each item of the total 126 has been transformed into a 16 x 1 latent feature vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK (Optional): Customize the RecommenderNet to potentially improve the model performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-defined `RecommenderNet()` is a actually very basic neural network, you are encouraged to customize it to see if model prediction performance will be improved. Here are some directions:\n",
    "\n",
    "*   Hyperparameter tuning, such as the embedding layer dimensions\n",
    "*   Add more hidden layers\n",
    "*   Try different activation functions such as `ReLu`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE YOUR CODE HERE\n",
    "\n",
    "## Update RecommenderNet() class\n",
    "\n",
    "## compile and fit the updated model\n",
    "\n",
    "## evaluate the updated model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you have learned and practiced predicting course ratings using neural networks. With a predefined and trained neural network, we can extract or embed users and items into latent feature spaces and further predict the interaction between a user and an item with the latent feature vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Yan Luo](https://www.linkedin.com/in/yan-luo-96288783/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML321ENSkillsNetwork32585014-2022-01-01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Date (YYYY-MM-DD) | Version | Changed By | Change Description          |\n",
    "| ----------------- | ------- | ---------- | --------------------------- |\n",
    "| 2021-10-25        | 1.0     | Yan        | Created the initial version |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright  2021 IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibm",
   "language": "python",
   "name": "ibm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
